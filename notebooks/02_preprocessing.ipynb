{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. 전처리 (Preprocessing)\n",
    "\n",
    "SAPA 데이터의 품질 관리(QC)와 성격 척도 점수를 계산합니다.\n",
    "\n",
    "## 학습 목표\n",
    "- 응답 품질 관리 (QC) 방법 이해\n",
    "- 역채점 처리 방법 이해\n",
    "- Big Five, Ideology, Honesty-Humility 점수 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치 (처음 한 번만 실행)\n",
    "%pip install pandas numpy scipy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 임포트\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "# 상위 폴더로 이동해서 데이터 접근\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('..')\n",
    "print(f'작업 폴더: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv('data/raw/sapa_data.csv')\n",
    "keys = pd.read_csv('data/raw/superKey696.csv', index_col=0)\n",
    "\n",
    "print(f'원본 데이터: {len(df):,}명')\n",
    "print(f'채점 키: {len(keys)}개 문항, {len(keys.columns)}개 척도')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성격 문항 컬럼 추출\n",
    "item_cols = [col for col in df.columns if col.startswith('q_')]\n",
    "print(f'성격 문항: {len(item_cols)}개')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 품질 관리 (Quality Control)\n",
    "\n",
    "### QC 기준\n",
    "1. **응답 부족**: 10개 미만 응답자 제외\n",
    "2. **Straight-lining**: 모든 응답이 동일한 경우 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC 1: 응답 부족 (10개 미만)\n",
    "responses_per_person = df[item_cols].notna().sum(axis=1)\n",
    "insufficient_response = responses_per_person < 10\n",
    "print(f'응답 부족 (10개 미만): {insufficient_response.sum():,}명')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC 2: Straight-lining (모든 응답이 동일)\n",
    "def check_straightlining(row):\n",
    "    valid_responses = row.dropna()\n",
    "    if len(valid_responses) < 2:\n",
    "        return False\n",
    "    return valid_responses.nunique() == 1\n",
    "\n",
    "straightlining = df[item_cols].apply(check_straightlining, axis=1)\n",
    "print(f'Straight-lining: {straightlining.sum():,}명')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC 적용\n",
    "qc_exclude = insufficient_response | straightlining\n",
    "df_clean = df[~qc_exclude].copy()\n",
    "\n",
    "print(f'\\n=== QC 결과 ===')\n",
    "print(f'원본: {len(df):,}명')\n",
    "print(f'제외: {qc_exclude.sum():,}명')\n",
    "print(f'유효: {len(df_clean):,}명')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 척도 점수 계산 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scale_score(df, keys, scale_name):\n",
    "    \"\"\"\n",
    "    채점 키를 사용해 척도 점수 계산\n",
    "    - 1: 정채점, -1: 역채점 (7 - 원점수), 0: 해당 없음\n",
    "    \"\"\"\n",
    "    # 해당 척도에 속하는 문항 찾기\n",
    "    scale_items = keys.index[keys[scale_name] != 0].tolist()\n",
    "    weights = keys.loc[scale_items, scale_name]\n",
    "    \n",
    "    # 데이터에 있는 문항만 필터링\n",
    "    available_items = [q for q in scale_items if q in df.columns]\n",
    "    \n",
    "    if not available_items:\n",
    "        return pd.Series([np.nan] * len(df), index=df.index)\n",
    "    \n",
    "    # 역채점 적용 (6점 척도: 7 - 원점수)\n",
    "    subset = df[available_items].copy()\n",
    "    for item in available_items:\n",
    "        if weights[item] == -1:\n",
    "            subset[item] = 7 - subset[item]\n",
    "    \n",
    "    # 평균 계산 (결측 무시)\n",
    "    return subset.mean(axis=1, skipna=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Big Five 점수 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big Five 점수 계산\n",
    "scores = pd.DataFrame()\n",
    "scores['RID'] = df_clean['RID']\n",
    "scores['state'] = df_clean['state']  # 나중에 State 분석용\n",
    "\n",
    "# NEO Big Five (약어 사용!)\n",
    "scores['NEO_O'] = calculate_scale_score(df_clean, keys, 'NEO_O').values\n",
    "scores['NEO_C'] = calculate_scale_score(df_clean, keys, 'NEO_C').values\n",
    "scores['NEO_E'] = calculate_scale_score(df_clean, keys, 'NEO_E').values\n",
    "scores['NEO_A'] = calculate_scale_score(df_clean, keys, 'NEO_A').values\n",
    "scores['NEO_N'] = calculate_scale_score(df_clean, keys, 'NEO_N').values\n",
    "\n",
    "print('Big Five 점수 계산 완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big Five 기술통계\n",
    "big_five_cols = ['NEO_O', 'NEO_C', 'NEO_E', 'NEO_A', 'NEO_N']\n",
    "big_five_names = ['Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Neuroticism']\n",
    "\n",
    "print('=== Big Five 기술통계 ===')\n",
    "for col, name in zip(big_five_cols, big_five_names):\n",
    "    n = scores[col].notna().sum()\n",
    "    mean = scores[col].mean()\n",
    "    sd = scores[col].std()\n",
    "    print(f'{name}: N={n:,}, M={mean:.2f}, SD={sd:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ideology 점수 계산\n",
    "\n",
    "**공식**: Ideology = mean( z(MPQ_Traditionalism), z(NEO_Liberalism) * -1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideology 구성 요소 계산\n",
    "scores['MPQ_Traditionalism'] = calculate_scale_score(df_clean, keys, 'MPQtr').values\n",
    "scores['NEO_Liberalism'] = calculate_scale_score(df_clean, keys, 'NEOo6').values\n",
    "\n",
    "# 유효한 값만 추출하여 z-score 계산 (index 유지 중요!)\n",
    "valid_mask = scores['MPQ_Traditionalism'].notna() & scores['NEO_Liberalism'].notna()\n",
    "\n",
    "if valid_mask.sum() > 0:\n",
    "    mpqtr_valid = scores.loc[valid_mask, 'MPQ_Traditionalism']\n",
    "    neo_lib_valid = scores.loc[valid_mask, 'NEO_Liberalism']\n",
    "    \n",
    "    # pd.Series로 변환하여 index 유지\n",
    "    z_mpqtr = pd.Series(stats.zscore(mpqtr_valid.values), index=mpqtr_valid.index)\n",
    "    z_neo_lib = pd.Series(stats.zscore(neo_lib_valid.values), index=neo_lib_valid.index)\n",
    "    \n",
    "    scores['Ideology'] = (z_mpqtr + z_neo_lib * -1) / 2\n",
    "\n",
    "n = scores['Ideology'].notna().sum()\n",
    "mean = scores['Ideology'].mean()\n",
    "sd = scores['Ideology'].std()\n",
    "print(f'Ideology: N={n:,}, M={mean:.2f}, SD={sd:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Honesty-Humility 점수 계산\n",
    "\n",
    "**공식**: H-H = mean( z(NEO_Morality[A2]), z(NEO_Modesty[A4]), z(HEXACO_H) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Honesty-Humility 구성 요소 계산\n",
    "scores['NEO_Morality'] = calculate_scale_score(df_clean, keys, 'NEOa2').values\n",
    "scores['NEO_Modesty'] = calculate_scale_score(df_clean, keys, 'NEOa4').values\n",
    "scores['HEXACO_H'] = calculate_scale_score(df_clean, keys, 'HEXACO_H').values\n",
    "\n",
    "# 세 구성 요소가 모두 유효한 경우에만 계산\n",
    "valid_mask = (scores['NEO_Morality'].notna() & \n",
    "              scores['NEO_Modesty'].notna() & \n",
    "              scores['HEXACO_H'].notna())\n",
    "\n",
    "if valid_mask.sum() > 0:\n",
    "    neo_a2_valid = scores.loc[valid_mask, 'NEO_Morality']\n",
    "    neo_a4_valid = scores.loc[valid_mask, 'NEO_Modesty']\n",
    "    hexaco_h_valid = scores.loc[valid_mask, 'HEXACO_H']\n",
    "    \n",
    "    z_neo_a2 = pd.Series(stats.zscore(neo_a2_valid.values), index=neo_a2_valid.index)\n",
    "    z_neo_a4 = pd.Series(stats.zscore(neo_a4_valid.values), index=neo_a4_valid.index)\n",
    "    z_hexaco_h = pd.Series(stats.zscore(hexaco_h_valid.values), index=hexaco_h_valid.index)\n",
    "    \n",
    "    scores['Honesty_Humility'] = (z_neo_a2 + z_neo_a4 + z_hexaco_h) / 3\n",
    "\n",
    "n = scores['Honesty_Humility'].notna().sum()\n",
    "mean = scores['Honesty_Humility'].mean()\n",
    "sd = scores['Honesty_Humility'].std()\n",
    "print(f'Honesty-Humility: N={n:,}, M={mean:.2f}, SD={sd:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed 폴더 생성\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# 점수 저장\n",
    "scores.to_csv('data/processed/sapa_scores.csv', index=False)\n",
    "print(f'저장 완료: data/processed/sapa_scores.csv')\n",
    "print(f'저장된 응답자 수: {len(scores):,}명')\n",
    "print(f'저장된 컬럼: {list(scores.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 최종 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== 전처리 완료 요약 ===')\n",
    "print(f'\\n[QC 결과]')\n",
    "print(f'원본: {len(df):,}명')\n",
    "print(f'제외: {qc_exclude.sum():,}명 (응답부족: {insufficient_response.sum()}, Straight-lining: {straightlining.sum()})')\n",
    "print(f'유효: {len(df_clean):,}명')\n",
    "\n",
    "print(f'\\n[척도별 기술통계]')\n",
    "all_scales = ['NEO_O', 'NEO_C', 'NEO_E', 'NEO_A', 'NEO_N', 'Ideology', 'Honesty_Humility']\n",
    "all_names = ['Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Neuroticism', 'Ideology', 'Honesty-Humility']\n",
    "\n",
    "for col, name in zip(all_scales, all_names):\n",
    "    n = scores[col].notna().sum()\n",
    "    mean = scores[col].mean()\n",
    "    sd = scores[col].std()\n",
    "    print(f'{name}: N={n:,}, M={mean:.2f}, SD={sd:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
